{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Generate Candidates\n",
    "\n",
    "Our goal during this step is to generate candidate mids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils.connect import get_connection \n",
    "from scripts.utils.data import FB2M_NAME_TABLE\n",
    "\n",
    "connection = get_connection()\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e594ba5f71b043ef95f5882d825fa13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>end_index</th>\n",
       "      <th>object</th>\n",
       "      <th>predicted_question_tokens</th>\n",
       "      <th>predicted_subject_names</th>\n",
       "      <th>question</th>\n",
       "      <th>question_tokens</th>\n",
       "      <th>relation</th>\n",
       "      <th>start_index</th>\n",
       "      <th>subject</th>\n",
       "      <th>subject_name</th>\n",
       "      <th>subject_name_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6219</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0bs56bp</td>\n",
       "      <td>[name, an, american, thoroughbread, racehorse]</td>\n",
       "      <td>[{'name': 'american thoroughbread', 'score': 1...</td>\n",
       "      <td>Name an American Thoroughbread racehorse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>biology/organism_classification/organisms_of_t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>03k3r</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364</th>\n",
       "      <td>9.0</td>\n",
       "      <td>01sjng</td>\n",
       "      <td>[what, kind, of, game, is, vision, racing, dri...</td>\n",
       "      <td>[{'name': 'vision racing driving simulator', '...</td>\n",
       "      <td>what kind of game is vision racing driving sim...</td>\n",
       "      <td>[what, kind, of, game, is, vision, racing, dri...</td>\n",
       "      <td>cvg/computer_videogame/cvg_genre</td>\n",
       "      <td>5.0</td>\n",
       "      <td>02qlppc</td>\n",
       "      <td>vision racing driving simulator</td>\n",
       "      <td>(vision, racing, driving, simulator)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9374</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0dlmm88</td>\n",
       "      <td>[what, tv, program, is, romance, film]</td>\n",
       "      <td>[{'name': 'romance film', 'score': 28.02931404...</td>\n",
       "      <td>what tv program is romance film</td>\n",
       "      <td>[what, tv, program, is, romance, film]</td>\n",
       "      <td>tv/tv_genre/programs</td>\n",
       "      <td>4.0</td>\n",
       "      <td>02l7c8</td>\n",
       "      <td>romance film</td>\n",
       "      <td>(romance, film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>4.0</td>\n",
       "      <td>04rrx</td>\n",
       "      <td>[what, state, is, polaski, located, in]</td>\n",
       "      <td>[{'name': 'polaski', 'score': 32.1325416564941...</td>\n",
       "      <td>what state is  polaski located in</td>\n",
       "      <td>[what, state, is, polaski, located, in]</td>\n",
       "      <td>location/location/containedby</td>\n",
       "      <td>3.0</td>\n",
       "      <td>049_zj3</td>\n",
       "      <td>polaski</td>\n",
       "      <td>(polaski,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0qcr0</td>\n",
       "      <td>[what, disease, claimed, the, life, of, fern, ...</td>\n",
       "      <td>[{'name': 'fern emmett', 'score': 23.679399490...</td>\n",
       "      <td>what disease claimed the life of fern emmett</td>\n",
       "      <td>[what, disease, claimed, the, life, of, fern, ...</td>\n",
       "      <td>people/deceased_person/cause_of_death</td>\n",
       "      <td>6.0</td>\n",
       "      <td>02w9ycr</td>\n",
       "      <td>fern emmett</td>\n",
       "      <td>(fern, emmett)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       end_index   object                          predicted_question_tokens  \\\n",
       "6219         NaN  0bs56bp     [name, an, american, thoroughbread, racehorse]   \n",
       "3364         9.0   01sjng  [what, kind, of, game, is, vision, racing, dri...   \n",
       "9374         6.0  0dlmm88             [what, tv, program, is, romance, film]   \n",
       "10142        4.0    04rrx            [what, state, is, polaski, located, in]   \n",
       "97           8.0    0qcr0  [what, disease, claimed, the, life, of, fern, ...   \n",
       "\n",
       "                                 predicted_subject_names  \\\n",
       "6219   [{'name': 'american thoroughbread', 'score': 1...   \n",
       "3364   [{'name': 'vision racing driving simulator', '...   \n",
       "9374   [{'name': 'romance film', 'score': 28.02931404...   \n",
       "10142  [{'name': 'polaski', 'score': 32.1325416564941...   \n",
       "97     [{'name': 'fern emmett', 'score': 23.679399490...   \n",
       "\n",
       "                                                question  \\\n",
       "6219           Name an American Thoroughbread racehorse    \n",
       "3364   what kind of game is vision racing driving sim...   \n",
       "9374                     what tv program is romance film   \n",
       "10142                  what state is  polaski located in   \n",
       "97          what disease claimed the life of fern emmett   \n",
       "\n",
       "                                         question_tokens  \\\n",
       "6219                                                 NaN   \n",
       "3364   [what, kind, of, game, is, vision, racing, dri...   \n",
       "9374              [what, tv, program, is, romance, film]   \n",
       "10142            [what, state, is, polaski, located, in]   \n",
       "97     [what, disease, claimed, the, life, of, fern, ...   \n",
       "\n",
       "                                                relation  start_index  \\\n",
       "6219   biology/organism_classification/organisms_of_t...          NaN   \n",
       "3364                    cvg/computer_videogame/cvg_genre          5.0   \n",
       "9374                                tv/tv_genre/programs          4.0   \n",
       "10142                      location/location/containedby          3.0   \n",
       "97                 people/deceased_person/cause_of_death          6.0   \n",
       "\n",
       "       subject                     subject_name  \\\n",
       "6219     03k3r                              NaN   \n",
       "3364   02qlppc  vision racing driving simulator   \n",
       "9374    02l7c8                     romance film   \n",
       "10142  049_zj3                          polaski   \n",
       "97     02w9ycr                      fern emmett   \n",
       "\n",
       "                        subject_name_tokens  \n",
       "6219                                    NaN  \n",
       "3364   (vision, racing, driving, simulator)  \n",
       "9374                        (romance, film)  \n",
       "10142                            (polaski,)  \n",
       "97                           (fern, emmett)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "tqdm_notebook().pandas()\n",
    "\n",
    "df = pd.read_pickle('step_1_predict_subject_name.pkl')\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define text preprocessing the same as the training data and step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ../../scripts/Simple QA Models/Subject Recognition Data.ipynb\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import scripts.utils.import_notebook\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "STEMMER = SnowballStemmer(\"english\")\n",
    "PREPROCESS = importlib.import_module(\n",
    "                \"scripts.Simple QA Models.Subject Recognition Data\").preprocess\n",
    "TOKENIZE = importlib.import_module(\n",
    "                \"scripts.Simple QA Models.Subject Recognition Data\").spacy_tokenize\n",
    "\n",
    "def text_preprocess(s):\n",
    "    # Define `text_preprocess` the way the input text was preprocessed before step 1\n",
    "    s = PREPROCESS(s)\n",
    "    s = TOKENIZE(s)\n",
    "    s = ' '.join(s)\n",
    "    return s\n",
    "\n",
    "def text_normalize_punctuation(s):\n",
    "    s = text_preprocess(s)\n",
    "    # In `Normalized Reference Resolution#HYPOTHESIS - Subject Name not in Question.ipynb` we found that\n",
    "    # aliases and questions match up more if punctuation is removed.\n",
    "    # Remove punctuation\n",
    "    s = re.sub(r'[^\\w\\s]','',s)\n",
    "    # Removing characters can create gaps of multiple spaces\n",
    "    # Substitue multiple spaces with one\n",
    "    s = re.sub('\\s+', ' ', s)\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def text_normalize_punctuation_stem(s):\n",
    "    s = text_preprocess(s)\n",
    "    \n",
    "    # Remove Possessives\n",
    "    tokens = s.split()\n",
    "    possessives = [\"'s\"]\n",
    "    tokens = [t for t in tokens if t not in possessives]\n",
    "    # Stem\n",
    "    tokens = [STEMMER.stem(t) for t in tokens]\n",
    "    s = ' '.join(tokens)\n",
    "    \n",
    "    s = text_normalize_punctuation(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Subject Aliases\n",
    "\n",
    "Create an index of subject aliases that are preprocessed similar to the predicted subect name. Allowing for a database lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "column \"alias_normalized_punctuation\" of relation \"fb_two_subject_name\" already exists\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3028ae404f6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ALTER TABLE '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mFB2M_NAME_TABLE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' ADD COLUMN alias_normalized_punctuation varchar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mProgrammingError\u001b[0m: column \"alias_normalized_punctuation\" of relation \"fb_two_subject_name\" already exists\n"
     ]
    }
   ],
   "source": [
    "cursor.execute('ALTER TABLE ' + FB2M_NAME_TABLE + ' ADD COLUMN alias_normalized_punctuation varchar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('ALTER TABLE ' + FB2M_NAME_TABLE + ' ADD COLUMN alias_normalized_punctuation_stem varchar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('ALTER TABLE ' + FB2M_NAME_TABLE + ' ADD COLUMN alias_preprocessed varchar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import psycopg2\n",
    "\n",
    "chunk_size = 10000\n",
    "\n",
    "def update_chunk(rows):\n",
    "    query = ('UPDATE ' + FB2M_NAME_TABLE + ' SET alias_preprocessed = %s, ' +\n",
    "             'alias_normalized_punctuation = %s, alias_normalized_punctuation_stem = %s ' +\n",
    "             'WHERE mid = %s and alias = %s')\n",
    "    psycopg2.extras.execute_batch(cursor, query, rows)\n",
    "\n",
    "cursor.execute('SELECT mid, alias FROM ' + FB2M_NAME_TABLE)\n",
    "rows = []\n",
    "for mid, alias in tqdm_notebook(cursor.fetchall()):\n",
    "    alias_preprocessed = text_preprocess(alias)\n",
    "    alias_normalized_punctuation = text_normalize_punctuation(alias)\n",
    "    alias_normalized_punctuation_stem = text_normalize_punctuation_stem(alias)\n",
    "    rows.append(tuple([alias_preprocessed, alias_normalized_punctuation, \n",
    "                       alias_normalized_punctuation_stem, mid, alias]))\n",
    "    \n",
    "    # Insert Chunk\n",
    "    if len(rows) > chunk_size:\n",
    "        update_chunk(rows)\n",
    "        rows = []\n",
    "        \n",
    "update_chunk(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('CREATE INDEX ' + FB2M_NAME_TABLE + '_alias_preprocessed ON ' + \n",
    "               FB2M_NAME_TABLE + '(alias_preprocessed);')\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('CREATE INDEX ' + FB2M_NAME_TABLE + '_alias_normalized_punctuation ON ' + \n",
    "               FB2M_NAME_TABLE + '(alias_normalized_punctuation);')\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('CREATE INDEX ' + FB2M_NAME_TABLE + '_alias_normalized_punctuation_stem ON ' + \n",
    "               FB2M_NAME_TABLE + '(alias_normalized_punctuation_stem);')\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('CREATE INDEX ' + FB2M_NAME_TABLE + '_alias_normalized_punctuation_stem_trgm ON ' + \n",
    "               FB2M_NAME_TABLE + ' USING gist(alias_normalized_punctuation_stem gist_trgm_ops);')\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If subject name is null, then the question does not refer to the true alias. The example is then unanswerable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_answerable = df[df.subject_name.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics used to evaluate different versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_candidates(candidates_mids):\n",
    "    correct = 0\n",
    "    skipped = 0\n",
    "    expected_accuracy = 0\n",
    "    n_examples = df_answerable.shape[0]\n",
    "\n",
    "    for i, (_, row) in enumerate(df_answerable.iterrows()):\n",
    "        mids = candidates_mids[i]\n",
    "        if len(mids) == 0:\n",
    "            skipped += 1\n",
    "        elif row['subject'] in mids:\n",
    "            correct += 1\n",
    "            expected_accuracy += 1 / len(mids)\n",
    "        \n",
    "    print('Precision: %f [%d of %d]' %\n",
    "              (correct / (n_examples - skipped), correct, (n_examples - skipped)))\n",
    "    print('Recall: %f [%d of %d]' %\n",
    "              ((n_examples - skipped) / n_examples, (n_examples - skipped), n_examples))\n",
    "    print('Expected Guessing Accuracy: %f [%d of %d]' % \n",
    "              (expected_accuracy / n_examples, expected_accuracy, n_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic helper functions to run experiments quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=65536)\n",
    "def cached_alias_to_mid(text):\n",
    "    cursor.execute(\"SELECT mid FROM \" + FB2M_NAME_TABLE +  \n",
    "                  \" WHERE alias = %s\", (text,))\n",
    "    return list([r[0] for r in cursor.fetchall()])\n",
    "\n",
    "def cached_aliases_to_mids(aliases):\n",
    "    mids = []\n",
    "    for alias in aliases:\n",
    "        mids.extend(cached_alias_to_mid(alias))\n",
    "    return mids\n",
    "\n",
    "@lru_cache(maxsize=65536)\n",
    "def cached_alias_normalized_punctuation_to_alias(text):\n",
    "    cursor.execute(\"SELECT DISTINCT alias FROM \" + FB2M_NAME_TABLE + \n",
    "                  \" WHERE alias_normalized_punctuation = %s\", (text,))\n",
    "    return list([r[0] for r in cursor.fetchall()])\n",
    "\n",
    "@lru_cache(maxsize=65536)\n",
    "def cached_alias_preprocessed_to_alias(text):\n",
    "    cursor.execute(\"SELECT DISTINCT alias FROM \" + FB2M_NAME_TABLE + \n",
    "                  \" WHERE alias_preprocessed = %s\", (text,))\n",
    "    return list([r[0] for r in cursor.fetchall()])\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=65536)\n",
    "def cached_alias_normalized_punctuation_stem_to_alias(text):\n",
    "    cursor.execute(\"SELECT DISTINCT alias FROM \" + FB2M_NAME_TABLE + \n",
    "                  \" WHERE alias_normalized_punctuation_stem = %s\", (text,))\n",
    "    return list([r[0] for r in cursor.fetchall()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Candidates - Upperbound\n",
    "\n",
    "Here we use the true alias, to compute the upperbound of this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab455d6cb3314f849d223b694c98ecbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10648), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 1.000000 [10648 of 10648]\n",
      "Recall: 1.000000 [10648 of 10648]\n",
      "Expected Guessing Accuracy: 0.692400 [7372 of 10648]\n"
     ]
    }
   ],
   "source": [
    "candidates_mids = []\n",
    "\n",
    "for index, row in tqdm_notebook(df_answerable.iterrows(), total=df_answerable.shape[0]):\n",
    "    candidate_aliases = [row['subject_name']]\n",
    "    candidates_mids.append(cached_aliases_to_mids(candidate_aliases))        \n",
    "\n",
    "evaluate_candidates(candidates_mids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Candidates - Baseline\n",
    "\n",
    "Just lookup the top k predicted subject names in order until one is seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91ebe8494634c249e612f5e9366f34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10648), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 0.964420 [10246 of 10624]\n",
      "Recall: 0.997746 [10624 of 10648]\n",
      "Expected Guessing Accuracy: 0.659801 [7025 of 10648]\n"
     ]
    }
   ],
   "source": [
    "from scripts.utils.table import format_pipe_table\n",
    "\n",
    "negative_sample = []\n",
    "candidates_mids = []\n",
    "\n",
    "for index, row in tqdm_notebook(df_answerable.iterrows(), total=df_answerable.shape[0]):\n",
    "    for predicted in row['predicted_subject_names']:\n",
    "        candidate_aliases = cached_alias_preprocessed_to_alias(predicted['name'])\n",
    "    \n",
    "        if len(candidate_aliases) > 0:\n",
    "            candidates_mids.append(cached_aliases_to_mids(candidate_aliases))\n",
    "            break\n",
    "            \n",
    "    if len(candidate_aliases) == 0:\n",
    "        candidates_mids.append([])\n",
    "        \n",
    "\n",
    "evaluate_candidates(candidates_mids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1\n",
    "\n",
    "For the first version, we will try to follow the strategy in `Normalized Reference Resolution#HYPOTHESIS - Subject Name not in Question.ipynb` to link more aliases to questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.166667\n"
     ]
    }
   ],
   "source": [
    "# Helper method to play with the metric\n",
    "def pg_trgm_similarity(text, other_text):\n",
    "    cursor.execute('SELECT similarity(%s, %s);', (text, other_text))\n",
    "    similarity = cursor.fetchall()[0][0]\n",
    "    return similarity\n",
    "                   \n",
    "# TEST\n",
    "print(pg_trgm_similarity('hi', 'hey'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f16ccfe8da04d1691c5d8f7a4f3ce5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10648), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ../../scripts/Simple QA Numbers/HYPOTHESIS - Subject Name not in Question.ipynb\n",
      "\n",
      "Precision: 0.968524 [10308 of 10643]\n",
      "Recall: 0.999530 [10643 of 10648]\n",
      "Expected Guessing Accuracy: 0.664496 [7075 of 10648]\n",
      "Negative Sample:\n",
      "| Index | Strategy | Max Similarity | Preprocessed Subject Name | Predicted Alias | Considered Aliases | Question |\n",
      "| --- | --- | --- | --- | --- | --- | --- |\n",
      "| 0 | PREPROCESSED | 0.0 | short | documentary film | ['documentary film'] | Name a short documentary film released in 2011 |\n",
      "| 1 | PREPROCESSED | 0.722222 | red cloud 's war | the red | ['the red clouds war', 'red clouds war', 'the red clouds', 'clouds war', 'red clouds', 'the red'] | what was involved in the red clouds war? |\n",
      "| 2 | PREPROCESSED | 0.75 | corporation nation | nation | ['nation book', 'corporation nation book', 'the corporation nation book', 'nation'] | what subject is the corporation nation book about |\n",
      "| 3 | PREPROCESSED | 0.8 | peter 's point plantation | peters | ['peters point plantation', 'peters point', 'point plantation', 'peters'] | What is peters point plantation's architectural style |\n",
      "| 4 | PREPROCESSED | 0.0555556 | album | aaron carter | ['aaron carter'] | Name an album released by aaron carter |\n",
      "| 5 | PREPROCESSED | 1.0 | pillows & prayers : cherry red 1982–1983 | pillows & prayers : cherry red 1982 - 1983 | ['pillows & prayers : cherry red 1982 - 1983'] | What is the name of the track list for the release pillows & prayers: cherry red 1982-1983? |\n",
      "| 6 | PREPROCESSED | 0.5 | commune of luxembourg | luxembourg | ['luxembourg'] | which country is the commune of luxembourg in |\n",
      "| 7 | PREPROCESSED | 0.764706 | the hits album 6 | 6 | ['hits album 6', '6'] | what song was included in the hits album 6 |\n",
      "| 8 | PREPROCESSED | 0.588235 | between two women | two women | ['two women'] | what is about between two women |\n",
      "| 9 | PREPROCESSED | 0.782609 | battle of hudson 's bay | bay | ['battle of hudsons bay', 'of hudsons bay', 'battle of hudsons', 'the battle of hudsons bay', 'hudsons bay', 'did the battle of hudsons bay', 'battle of', 'bay'] | where did the battle of hudsons bay take place |\n",
      "| 10 | PREPROCESSED | 0.0 | tablet | hypertension | ['hypertension'] | what is a tablet used to treat hypertension  |\n",
      "| 11 | PREPROCESSED | 0.0 | compilation album | frank zappa | ['frank zappa'] | what compilation album did frank zappa release? |\n",
      "| 12 | PREPROCESSED | 0.0 | soundtrack | anthony marinelli | ['anthony marinelli'] | What's a soundtrack written by anthony marinelli |\n",
      "| 13 | PREPROCESSED | 0.0 | album | george canyon | ['george canyon'] | name an album by George Canyon |\n",
      "| 14 | PREPROCESSED | 0.0 | album | portal | ['portal'] | What's an album by the band portal |\n",
      "| 15 | PREPROCESSED | 0.785714 | megan cheng | megan | ['megan chengs', 'megan'] | whats  megan chengs ethnicity |\n",
      "| 16 | PREPROCESSED | 0.705882 | martial arts film | martial arts | ['martial arts'] | what is the name of the netflix martial arts film? |\n",
      "| 17 | PREPROCESSED | 0.0222222 | creedence clearwater revival | compilation album | ['compilation album'] | What is a compilation album by creedence clearwater revival |\n",
      "| 18 | PREPROCESSED | 0.227273 | topical medication | medicine | ['medicine'] | Name a topical medicine |\n",
      "| 19 | PREPROCESSED | 0.636364 | master | the master | ['the master'] | what is one of the master's powers  |\n",
      "| 20 | PREPROCESSED | 0.0 | t - town | kearny | ['kearny'] | What newspaper circulates in the town of kearny |\n",
      "| 21 | PREPROCESSED | 0.571429 | drums | drum | ['drum'] | which musician plays the drum kit |\n",
      "| 22 | PREPROCESSED | 0.84375 | dimillo 's floating restaurant | restaurant | ['dimillos floating restaurant', 'dimillos floating', 'floating restaurant', 'dimillos', 'is dimillos floating restaurant', 'dimillos floating restaurant in', 'restaurant'] | what state is dimillos floating restaurant in? |\n",
      "| 23 | PREPROCESSED | 0.0 | ragtime | denmark | ['denmark'] | who is the ragtime artist born in denmark? |\n",
      "| 24 | PREPROCESSED | 0.8 | the regatta mystery | mystery | ['regatta mystery', 'mystery'] | what theme is in the piece regatta mystery |\n",
      "| 25 | PREPROCESSED | 0.0 | album | jack | ['jack dejohnrette', 'jack'] | What is the name of Jack DeJohnrette's album? |\n",
      "| 26 | PREPROCESSED | 0.0 | bollywood | tamil | ['tamil'] | what bollywood Tamil film was released in 2004  |\n",
      "| 27 | PREPROCESSED | 0.0 | animated cartoon | ducks | ['ducks'] | what animated cartoon was about ducks? |\n",
      "| 28 | PREPROCESSED | 0.0 | photography | visual art | ['visual art'] | which artist uses photography as their preferred visual art form |\n",
      "| 29 | PREPROCESSED | 0.761905 | this pud 's for you | for you | ['this puds for you comes', 'this puds for you', 'this puds for you comes from', 'puds for you comes', 'puds for you', 'this puds for', 'this puds', 'episode this puds for you comes', 'for you comes', 'puds for you comes from', 'episode this puds for you', 'for you'] | what is the series where the episode this puds for you comes from |\n",
      "| 30 | NORMALIZED_PUNCTUATION | 0.826087 | chet 's speech , part ii | , part ii | ['chets speech , part ii', 'speech , part ii', 'chets speech , part', 'chets speech ,', 'chets speech', ', part ii'] | who sings chets speech, part ii |\n",
      "| 31 | PREPROCESSED | 0.764706 | large family car | family | ['large family', 'family'] | What car model is an example of a large family car? |\n",
      "| 32 | PREPROCESSED | 0.761905 | men 's pommel horse | pommel horse | ['mens pommel horse', 'mens pommel', 'pommel horse'] | What olympic games featured mens pommel horse |\n",
      "| 33 | NORMALIZED_PUNCTUATION | 0.0526316 | soundtrack | s.cry.ed | ['s.cry.ed'] | What's the soundtrack for s.cry.ed |\n",
      "| 34 | PREPROCESSED | 0.35 | sahara ( instrumental ) | sahara | ['sahara'] | who composed sahara (instrumental)? |\n",
      "| 35 | PREPROCESSED | 0.0625 | compilation | cema | ['albumby cema', 'cema'] | what album is released as a compilation albumby CEMA |\n",
      "| 36 | PREPROCESSED | 0.583333 | arabic name | arabic | ['arabic'] | What is a book that is about arabic name |\n",
      "| 37 | PREPROCESSED | 0.73913 | multiplayer video game | game | ['multiplayer game', 'game'] | What's a text based multiplayer game |\n",
      "| 38 | PREPROCESSED | 0.75 | the crystal city | crystal city | ['crystal city'] | what genre is crystal city |\n",
      "| 39 | PREPROCESSED | 0.84 | men 's badminton , singles | singles | ['mens badminton , singles', 'badminton , singles', 'mens badminton', 'mens badminton ,', 'singles'] | what olympic games was mens badminton, singles apart of |\n",
      "| 40 | PREPROCESSED | 0.0 | mercedes lackey | fantasy | ['fantasy'] | which fantasy series were written by mercedes lackey? |\n",
      "| 41 | PREPROCESSED | 0.666667 | brian o'shea | brian oshea | ['brian oshea'] | brian oshea performs what type of martial art |\n",
      "| 42 | PREPROCESSED | 0.857143 | u.s . office of war information | war | ['office of war information', 'office of war information help', 'the office of war information', 'war information', 'the office of war information help', 'of war information', 'office of war', 'war information help', 'of war information help', 'the office of war', 'office of war information help produce', 'war'] | which film did the office of war information help produce  |\n",
      "| 43 | PREPROCESSED | 0.0 | album | sham 69 | ['sham 69'] | which album is released by Sham 69 |\n",
      "| 44 | NORMALIZED_PUNCTUATION | 0.777778 | lowthian bell | , 1st baronet | ['sir lowthian bell , 1st baronet', 'lowthian bell , 1st baronet', 'sir lowthian bell , 1st', 'sir lowthian bell ,', 'sir lowthian bell', 'bell , 1st baronet', 'sir lowthian', ', 1st baronet'] | what organization was founded by sir lowthian bell, 1st baronet |\n",
      "| 45 | PREPROCESSED | 0.862069 | st . peter 's episcopal church | st . peters | ['st . peters episcopal church', 'st . peters episcopal', '. peters episcopal church', 'peters episcopal church', 'st . peters'] | what state and city is st. peters episcopal church located in? |\n",
      "| 46 | PREPROCESSED | 0.851852 | richard scarry 's busytown | busytown | ['richard scarrys busytown', 'scarrys busytown', 'richard scarrys', 'busytown'] | what is a gameplay mode featured on richard scarrys busytown |\n",
      "| 47 | PREPROCESSED | 0.0 | album | soil | ['soil'] | What's an album by soil |\n",
      "| 48 | PREPROCESSED | 0.714286 | texas a&m university school of law | texas wesleyan university | ['texas wesleyan university school of law', 'wesleyan university school of law', 'texas wesleyan university school of', 'texas wesleyan university school', 'university school of law', 'is texas wesleyan university school of law', 'texas wesleyan university'] | Where is texas wesleyan university school of law located? |\n",
      "| 49 | PREPROCESSED | 0.535714 | public service announcement | public service | ['public service'] | What is the name of a public service announcement? |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scripts.utils.table import format_pipe_table\n",
    "\n",
    "negative_samples = []\n",
    "candidates_mids = []\n",
    "\n",
    "for index, row in tqdm_notebook(df_answerable.iterrows(), total=df_answerable.shape[0]):\n",
    "    for i, predicted in enumerate(row['predicted_subject_names']):\n",
    "        strategy = 'PREPROCESSED'\n",
    "        candidate_aliases = cached_alias_preprocessed_to_alias(predicted['name'])\n",
    "        \n",
    "        # Punctuation Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            # NOTE: Normalized alias has a broader reach; therefore, we only use it if the first check failed.\n",
    "            # We found this increased precision and expected guessing accuracy to add the check.\n",
    "            strategy = 'NORMALIZED_PUNCTUATION'\n",
    "            candidate_aliases = cached_alias_normalized_punctuation_to_alias(\n",
    "                text_normalize_punctuation(predicted['name']))\n",
    "    \n",
    "        if len(candidate_aliases) > 0:\n",
    "            candidates_mids.append(cached_aliases_to_mids(candidate_aliases))\n",
    "            if row['subject'] not in candidates_mids[-1]:\n",
    "                considered_aliases = [predicted['name'] for j, predicted in \n",
    "                                          enumerate(row['predicted_subject_names']) if j <= i]\n",
    "                negative_samples.append({\n",
    "                    'Preprocessed Subject Name': text_preprocess(row['subject_name']),\n",
    "                    'Considered Aliases': considered_aliases,\n",
    "                    'Max Similarity': max([pg_trgm_similarity(row['subject_name'], a)\n",
    "                                           for a in considered_aliases]),\n",
    "                    'Predicted Alias': predicted['name'],\n",
    "                    'Strategy': strategy,\n",
    "                    'Question': row['question'],\n",
    "                })\n",
    "            break\n",
    "            \n",
    "    if len(candidate_aliases) == 0:\n",
    "        candidates_mids.append([])\n",
    "\n",
    "evaluate_candidates(candidates_mids)\n",
    "print('Negative Sample:')\n",
    "print(format_pipe_table(negative_samples[:50], columns=['Strategy', 'Max Similarity',\n",
    "                                                        'Preprocessed Subject Name',\n",
    "                                                        'Predicted Alias',\n",
    "                                                        'Considered Aliases', 'Question']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "##### Numbers:\n",
    "\n",
    "Version 0\n",
    "- Precision: 0.964420 [10246 of 10624]\n",
    "- Recall: 0.997746 [10624 of 10648]\n",
    "- Expected Guessing Accuracy: 0.659801 [7025 of 10648]\n",
    "\n",
    "Version 1\n",
    "- Precision: 0.968524 [10308 of 10643]\n",
    "- Recall: 0.999530 [10643 of 10648]\n",
    "- Expected Guessing Accuracy: 0.664496 [7075 of 10648]\n",
    "\n",
    "Recall increased by 0.001784.\n",
    "Precision increased by 0.004104.\n",
    "\n",
    "\n",
    "##### Error Bucket:\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "Handling possesives would fix 10 / 50 errors. Handling the `Similar` bucket would be difficult because it's typically because of extra words in the subject name not present in the question.\n",
    "\n",
    "**Buckets:**\n",
    "- Wrong Span (29 / 50): The wrong span in the question was selected\n",
    "- Suffix (12 / 50): The correct subject name was not linked due to a suffix.\n",
    "- Extra Article (3 / 50): The correct subject name was not linked due to an article.\n",
    "- Similar (7 / 50): The correct subject name was similar but not exact to the predicted subject name.\n",
    "- Other (1 / 50): Deeper reason that the correct subject name was not linked.\n",
    "\n",
    "| Index | Similarity | Bucket | Strategy | Preprocessed Subject Name | Predicted Alias | Considered Aliases | Question |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 0 | 0.0 | Wrong Span | PREPROCESSED | short | documentary film | ['documentary film'] | Name a short documentary film released in 2011 |\n",
    "| 1 | 0.722222 | Suffix | PREPROCESSED | red cloud 's war | the red | ['the red clouds war', 'red clouds war', 'the red clouds', 'clouds war', 'red clouds', 'the red'] | what was involved in the red clouds war? |\n",
    "| 2 | 0.75 | Wrong Span | PREPROCESSED | corporation nation | nation | ['nation book', 'corporation nation book', 'the corporation nation book', 'nation'] | what subject is the corporation nation book about |\n",
    "| 3 | 0.8 | Suffix | PREPROCESSED | peter 's point plantation | peters | ['peters point plantation', 'peters point', 'point plantation', 'peters'] | What is peters point plantation's architectural style |\n",
    "| 4 | 0.0555556 | Wrong Span | PREPROCESSED | album | aaron carter | ['aaron carter'] | Name an album released by aaron carter |\n",
    "| 5 | 1.0 | Similar | PREPROCESSED | pillows & prayers : cherry red 1982–1983 | pillows & prayers : cherry red 1982 - 1983 | ['pillows & prayers : cherry red 1982 - 1983'] | What is the name of the track list for the release pillows & prayers: cherry red 1982-1983? |\n",
    "| 6 | 0.5 | Wrong Span | PREPROCESSED | commune of luxembourg | luxembourg | ['luxembourg'] | which country is the commune of luxembourg in |\n",
    "| 7 | 0.764706 | Extra Article | PREPROCESSED | the hits album 6 | 6 | ['hits album 6', '6'] | what song was included in the hits album 6 |\n",
    "| 8 | 0.588235 | Wrong Span | PREPROCESSED | between two women | two women | ['two women'] | what is about between two women |\n",
    "| 9 | 0.782609 | Suffix | PREPROCESSED | battle of hudson 's bay | bay | ['battle of hudsons bay', 'of hudsons bay', 'battle of hudsons', 'the battle of hudsons bay', 'hudsons bay', 'did the battle of hudsons bay', 'battle of', 'bay'] | where did the battle of hudsons bay take place |\n",
    "| 10 | 0.0 | Wrong Span | PREPROCESSED | tablet | hypertension | ['hypertension'] | what is a tablet used to treat hypertension  |\n",
    "| 11 | 0.0 | Wrong Span | PREPROCESSED | compilation album | frank zappa | ['frank zappa'] | what compilation album did frank zappa release? |\n",
    "| 12 | 0.0 | Wrong Span | PREPROCESSED | soundtrack | anthony marinelli | ['anthony marinelli'] | What's a soundtrack written by anthony marinelli |\n",
    "| 13 | 0.0 | Wrong Span | PREPROCESSED | album | george canyon | ['george canyon'] | name an album by George Canyon |\n",
    "| 14 | 0.0 | Wrong Span | PREPROCESSED | album | portal | ['portal'] | What's an album by the band portal |\n",
    "| 15 | 0.785714 | Suffix | PREPROCESSED | megan cheng | megan | ['megan chengs', 'megan'] | whats  megan chengs ethnicity |\n",
    "| 16 | 0.705882 | Wrong Span | PREPROCESSED | martial arts film | martial arts | ['martial arts'] | what is the name of the netflix martial arts film? |\n",
    "| 17 | 0.0222222 | Wrong Span | PREPROCESSED | creedence clearwater revival | compilation album | ['compilation album'] | What is a compilation album by creedence clearwater revival |\n",
    "| 18 | 0.227273 | Wrong Span | PREPROCESSED | topical medication | medicine | ['medicine'] | Name a topical medicine |\n",
    "| 19 | 0.636364 | Wrong Span | PREPROCESSED | master | the master | ['the master'] | what is one of the master's powers  |\n",
    "| 20 | 0.0 | Other | PREPROCESSED | t - town | kearny | ['kearny'] | What newspaper circulates in the town of kearny |\n",
    "| 21 | 0.571429 | Suffix | PREPROCESSED | drums | drum | ['drum'] | which musician plays the drum kit |\n",
    "| 22 | 0.84375 | Suffix | PREPROCESSED | dimillo 's floating restaurant | restaurant | ['dimillos floating restaurant', 'dimillos floating', 'floating restaurant', 'dimillos', 'is dimillos floating restaurant', 'dimillos floating restaurant in', 'restaurant'] | what state is dimillos floating restaurant in? |\n",
    "| 23 | 0.0 | Wrong Span | PREPROCESSED | ragtime | denmark | ['denmark'] | who is the ragtime artist born in denmark? |\n",
    "| 24 | 0.8 | Similar, Extra Article | PREPROCESSED | the regatta mystery | mystery | ['regatta mystery', 'mystery'] | what theme is in the piece regatta mystery |\n",
    "| 25 | 0.0 | Wrong Span | PREPROCESSED | album | jack | ['jack dejohnrette', 'jack'] | What is the name of Jack DeJohnrette's album? |\n",
    "| 26 | 0.0 | Wrong Span | PREPROCESSED | bollywood | tamil | ['tamil'] | what bollywood Tamil film was released in 2004  |\n",
    "| 27 | 0.0 | Wrong Span | PREPROCESSED | animated cartoon | ducks | ['ducks'] | what animated cartoon was about ducks? |\n",
    "| 28 | 0.0 | Wrong Span | PREPROCESSED | photography | visual art | ['visual art'] | which artist uses photography as their preferred visual art form |\n",
    "| 29 | 0.761905 | Suffix | PREPROCESSED | this pud 's for you | for you | ['this puds for you comes', 'this puds for you', 'this puds for you comes from', 'puds for you comes', 'puds for you', 'this puds for', 'this puds', 'episode this puds for you comes', 'for you comes', 'puds for you comes from', 'episode this puds for you', 'for you'] | what is the series where the episode this puds for you comes from |\n",
    "| 30 | 0.826087 | Suffix | NORMALIZED | chet 's speech , part ii | , part ii | ['chets speech , part ii', 'speech , part ii', 'chets speech , part', 'chets speech ,', 'chets speech', ', part ii'] | who sings chets speech, part ii |\n",
    "| 31 | 0.764706 | Wrong Span | PREPROCESSED | large family car | family | ['large family', 'family'] | What car model is an example of a large family car? |\n",
    "| 32 | 0.761905 | Suffix | PREPROCESSED | men 's pommel horse | pommel horse | ['mens pommel horse', 'mens pommel', 'pommel horse'] | What olympic games featured mens pommel horse |\n",
    "| 33 | 0.0526316 | Wrong Span | NORMALIZED | soundtrack | s.cry.ed | ['s.cry.ed'] | What's the soundtrack for s.cry.ed |\n",
    "| 34 | 0.35 | Wrong Span | PREPROCESSED | sahara ( instrumental ) | sahara | ['sahara'] | who composed sahara (instrumental)? |\n",
    "| 35 | 0.0625 | Wrong Span | PREPROCESSED | compilation | cema | ['albumby cema', 'cema'] | what album is released as a compilation albumby CEMA |\n",
    "| 36 | 0.583333 | Wrong Span | PREPROCESSED | arabic name | arabic | ['arabic'] | What is a book that is about arabic name |\n",
    "| 37 | 0.73913 | Similar | PREPROCESSED | multiplayer video game | game | ['multiplayer game', 'game'] | What's a text based multiplayer game |\n",
    "| 38 | 0.75 | Extra Article | PREPROCESSED | the crystal city | crystal city | ['crystal city'] | what genre is crystal city |\n",
    "| 39 | 0.84 | Suffix | PREPROCESSED | men 's badminton , singles | singles | ['mens badminton , singles', 'badminton , singles', 'mens badminton', 'mens badminton ,', 'singles'] | what olympic games was mens badminton, singles apart of |\n",
    "| 40 | 0.0 | Wrong Span | PREPROCESSED | mercedes lackey | fantasy | ['fantasy'] | which fantasy series were written by mercedes lackey? |\n",
    "| 41 | 0.666667 | Similar | PREPROCESSED | brian o'shea | brian oshea | ['brian oshea'] | brian oshea performs what type of martial art |\n",
    "| 42 | 0.857143 | Similar | PREPROCESSED | u.s . office of war information | war | ['office of war information', 'office of war information help', 'the office of war information', 'war information', 'the office of war information help', 'of war information', 'office of war', 'war information help', 'of war information help', 'the office of war', 'office of war information help produce', 'war'] | which film did the office of war information help produce  |\n",
    "| 43 | 0.0 | Wrong Span | PREPROCESSED | album | sham 69 | ['sham 69'] | which album is released by Sham 69 |\n",
    "| 44 | 0.777778 | Wrong Span | NORMALIZED | lowthian bell | , 1st baronet | ['sir lowthian bell , 1st baronet', 'lowthian bell , 1st baronet', 'sir lowthian bell , 1st', 'sir lowthian bell ,', 'sir lowthian bell', 'bell , 1st baronet', 'sir lowthian', ', 1st baronet'] | what organization was founded by sir lowthian bell, 1st baronet |\n",
    "| 45 | 0.862069 | Suffix | PREPROCESSED | st . peter 's episcopal church | st . peters | ['st . peters episcopal church', 'st . peters episcopal', '. peters episcopal church', 'peters episcopal church', 'st . peters'] | what state and city is st. peters episcopal church located in? |\n",
    "| 46 | 0.851852 | Suffix | PREPROCESSED | richard scarry 's busytown | busytown | ['richard scarrys busytown', 'scarrys busytown', 'richard scarrys', 'busytown'] | what is a gameplay mode featured on richard scarrys busytown |\n",
    "| 47 | 0.0 | Wrong Span | PREPROCESSED | album | soil | ['soil'] | What's an album by soil |\n",
    "| 48 | 0.714286 | Similar | PREPROCESSED | texas a&m university school of law | texas wesleyan university | ['texas wesleyan university school of law', 'wesleyan university school of law', 'texas wesleyan university school of', 'texas wesleyan university school', 'university school of law', 'is texas wesleyan university school of law', 'texas wesleyan university'] | Where is texas wesleyan university school of law located? |\n",
    "| 49 | 0.535714 | Wrong Span | PREPROCESSED | public service announcement | public service | ['public service'] | What is the name of a public service announcement? |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2\n",
    "\n",
    "In version 1, the error bucketing revealed a failure to handle suffix's; therefore, we proceed to handle them in version 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aeb39dd43e941e9a3e9fe0189c7897b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10648), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 0.973420 [10364 of 10647]\n",
      "Recall: 0.999906 [10647 of 10648]\n",
      "Expected Guessing Accuracy: 0.669337 [7127 of 10648]\n",
      "Negative Sample:\n",
      "| Index | Strategy | Max Similarity | Preprocessed Subject Name | Predicted Alias | Considered Aliases | Question |\n",
      "| --- | --- | --- | --- | --- | --- | --- |\n",
      "| 0 | PREPROCESSED | 0.705882 | pim fortuyn list | pim fortuyn | ['pim fortuyn'] | what ideology does the pim fortuyn list follow |\n",
      "| 1 | NORMALIZED_PUNCTUATION_STEM | 0.0 | drug | cleaning | ['cleaning hands', 'for cleaning hands', 'cleaning'] | what drug is used for cleaning hands  |\n",
      "| 2 | PREPROCESSED | 0.444444 | leather subculture | leather | ['leather'] | what's one event that celebrates the leather subculture |\n",
      "| 3 | PREPROCESSED | 0.333333 | illinois river | rogue river | ['lower rogue river', 'rogue river'] | in which community does the illinois river confluence with the Lower Rogue river |\n",
      "| 4 | PREPROCESSED | 0.764706 | need for speed | the need for speed | ['the need for speed'] | what type of film is the need for speed |\n",
      "| 5 | PREPROCESSED | 0.0 | asteroid | geologist | ['geologist'] | which asteroid was names after an italian geologist? |\n",
      "| 6 | PREPROCESSED | 0.0 | album | chico debarge | ['chico debarge'] | what is an album by Chico DeBarge? |\n",
      "| 7 | PREPROCESSED | 1.0 | unter null . | unter null | ['unter null'] | what type of book binding is unter null. |\n",
      "| 8 | NORMALIZED_PUNCTUATION | 0.5 | k - pop | - pop music | ['k - pop music', '- pop music'] | Who is an artist of k-pop music? |\n",
      "| 9 | PREPROCESSED | 0.0454545 | novel | vladimir nabokov | ['vladimir nabokov'] | Name a novel by Vladimir Nabokov |\n",
      "| 10 | PREPROCESSED | 0.888889 | millard s drexler | millard | ['millard drexler', 'drexler', 'millard'] | What organization did millard drexler found |\n",
      "| 11 | PREPROCESSED | 0.0 | musical | israel | ['israel'] | which musical films were broadcasted in israel? |\n",
      "| 12 | PREPROCESSED | 0.0 | live album | 3oh!3 | ['3oh!3'] | what is the name of the live album by 3oh!3 |\n",
      "| 13 | PREPROCESSED | 0.5 | epic film | epic | ['epic'] | Name a 1936 epic film  |\n",
      "| 14 | PREPROCESSED | 0.533333 | roy rogers restaurants | roy rogers | ['roy rogers'] | roy rogers restaurants in which industry? |\n",
      "| 15 | PREPROCESSED | 0.0 | caucasian | babylon 5 | ['babylon 5'] | who is of caucasian race in babylon 5 |\n",
      "| 16 | PREPROCESSED | 0.6 | painting | visual art | ['visual art painting', 'art painting', 'visual art'] | what is a visual art painting |\n",
      "| 17 | PREPROCESSED | 0.0 | album | funk | ['koul funk', 'funk'] | what album was release by Koul Funk |\n",
      "| 18 | PREPROCESSED | 0.0555556 | science | brian swimme | ['brian swimme'] | brian swimme wrote what book that dealt with  science |\n",
      "| 19 | PREPROCESSED | 0.0 | animation | raoul servais | ['raoul servais'] | Which animation did Raoul Servais direct |\n",
      "| 20 | PREPROCESSED | 0.333333 | working title films | films | ['films'] | what is the film from the production company working title films |\n",
      "| 21 | PREPROCESSED | 0.0344828 | jerry bruckheimer | biographical film | ['biographical film'] | jerry bruckheimer was the producer of this biographical film.  |\n",
      "| 22 | PREPROCESSED | 0.714286 | godbout v longueuil ( city of ) | longueuil | ['godbout v. longueuil', 'godbout v.', 'v. longueuil', 'the godbout v. longueuil', 'godbout v. longueuil case', 'godbout', 'longueuil'] | what court handled the godbout v. longueuil case? |\n",
      "| 23 | PREPROCESSED | 0.875 | raymond a. meier | raymond | ['raymond meier', 'meier', 'was raymond meier', 'raymond'] | Which city was raymond meier born in |\n",
      "| 24 | PREPROCESSED | 0.714286 | afterglow | the afterglow | ['the afterglow fil', 'the afterglow fil ,', 'afterglow fil', 'afterglow fil ,', 'the afterglow'] | who did the music for the afterglow fil, |\n",
      "| 25 | PREPROCESSED | 0.0 | album | leo sayer | ['leo sayer'] | what is an album by leo sayer? |\n",
      "| 26 | PREPROCESSED | 1.0 | drums | drum | ['drum'] | who played the drum  in the Los Angeles rock quintet Rooney |\n",
      "| 27 | PREPROCESSED | 0.625 | latin pop | pop music | ['latin pop music', 'pop music'] | who is an artist that creates latin pop music |\n",
      "| 28 | PREPROCESSED | 0.0909091 | death eaters | harry potter | ['harry potter'] | which is the name of a death eater in harry potter? |\n",
      "| 29 | NORMALIZED_PUNCTUATION | 0.62069 | single - player video game | single - | ['single - player mode', 'single - player mode game', 'single - player', 'single -'] | what is a single-player mode game? |\n",
      "| 30 | PREPROCESSED | 0.461538 | album | compilation album | ['compilation album'] | What is a compilation album from 2006  |\n",
      "| 31 | PREPROCESSED | 0.047619 | studio album | arcangel | ['arcangel'] | What was a studio album recording for Arcángel |\n",
      "| 32 | PREPROCESSED | 0.6 | saint | the saint | ['the saint novel', 'saint novel', 'the saint'] | What type of book is the saint novel? |\n",
      "| 33 | PREPROCESSED | 0.5 | avila place | avila | ['avila'] | what western state does contain avila place |\n",
      "| 34 | PREPROCESSED | 0.0357143 | north carolina | surrey | ['surrey county', 'surrey'] | What is a city in Surrey County, north carolina? |\n",
      "| 35 | PREPROCESSED | 0.0 | album | johannes brahms | ['johannes brahms'] | What is an album written by Johannes Brahms |\n",
      "| 36 | PREPROCESSED | 0.777778 | first battle of james island | james island | ['battle of james island', 'battle of james', 'of james island', 'james island'] | Name a soldier involved in the battle of james island. |\n",
      "| 37 | PREPROCESSED | 0.642857 | plymouth | plymouth rock | ['plymouth rock'] | is there another attraction in plymouth other than plymouth rock |\n",
      "| 38 | PREPROCESSED | 0.705882 | altered beast | beast | ['beast game', 'altered beast game', 'beast'] | who is the creator of the altered beast game |\n",
      "| 39 | PREPROCESSED | 0.45 | the barefoot artist | barefoot | ['barefoot'] | which film created the barefoot artist  |\n",
      "| 40 | PREPROCESSED | 0.352941 | pornographic actor | actor | ['actor'] | who is a pornographic actor |\n",
      "| 41 | PREPROCESSED | 0.0 | album | century media | ['century media'] | which albums were released by the century media label? |\n",
      "| 42 | PREPROCESSED | 0.5 | 8833 acer | acer | ['acer'] | what is a 8833 acer |\n",
      "| 43 | PREPROCESSED | 1.0 | cruisin ' | cruisin | ['cruisin'] | What release is cruisin on? |\n",
      "| 44 | NORMALIZED_PUNCTUATION_STEM | 0.88 | the wonderful wizard of ha 's | the wonderful | ['the wonderful wizard of', 'the wonderful wizard of has', 'the wonderful wizard', 'wonderful wizard of', 'wonderful wizard of has', 'wonderful wizard', 'wizard of', 'the wonderful'] | What film series is the wonderful wizard of has from? |\n",
      "| 45 | PREPROCESSED | 0.842105 | tower of london | the tower of london | ['the tower of london'] | who recorded the tower of london |\n",
      "| 46 | PREPROCESSED | 0.7 | outside in | outside | ['outside'] | Which genre is outside in associated with |\n",
      "| 47 | PREPROCESSED | 0.4 | rca | rca records | ['rca records'] | Who is an artist  signed by rca records? |\n",
      "| 48 | PREPROCESSED | 0.0 | action game | sega | ['sega'] | What's an action game made by sega |\n",
      "| 49 | PREPROCESSED | 0.647059 | film adaptation | novel | ['novel film adaptation', 'novel'] | What's an example of a novel film adaptation |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scripts.utils.table import format_pipe_table\n",
    "\n",
    "negative_samples = []\n",
    "candidates_mids = []\n",
    "\n",
    "for index, row in tqdm_notebook(df_answerable.iterrows(), total=df_answerable.shape[0]):\n",
    "    for i, predicted in enumerate(row['predicted_subject_names']):\n",
    "        strategy = 'PREPROCESSED'\n",
    "        candidate_aliases = cached_alias_preprocessed_to_alias(predicted['name'])\n",
    "        \n",
    "        # Punctuation Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            # NOTE: Normalized alias has a broader reach; therefore, we only use it if the first check failed.\n",
    "            # We found this increased precision and expected guessing accuracy to add the check.\n",
    "            strategy = 'NORMALIZED_PUNCTUATION'\n",
    "            candidate_aliases = cached_alias_normalized_punctuation_to_alias(\n",
    "                text_normalize_punctuation(predicted['name']))\n",
    "            \n",
    "        # Punctuation Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            # NOTE: Normalized alias has a broader reach; therefore, we only use it if the first check failed.\n",
    "            # We found this increased precision and expected guessing accuracy to add the check.\n",
    "            strategy = 'NORMALIZED_PUNCTUATION_STEM'\n",
    "            candidate_aliases = cached_alias_normalized_punctuation_stem_to_alias(\n",
    "                text_normalize_punctuation_stem(predicted['name']))\n",
    "    \n",
    "        if len(candidate_aliases) > 0:\n",
    "            candidates_mids.append(cached_aliases_to_mids(candidate_aliases))\n",
    "            if row['subject'] not in candidates_mids[-1]:\n",
    "                considered_aliases = [predicted['name'] for j, predicted in \n",
    "                                          enumerate(row['predicted_subject_names']) if j <= i]\n",
    "                negative_samples.append({\n",
    "                    'Preprocessed Subject Name': text_preprocess(row['subject_name']),\n",
    "                    'Considered Aliases': considered_aliases,\n",
    "                    'Max Similarity': max([pg_trgm_similarity(text_normalize_punctuation_stem(row['subject_name']),\n",
    "                                                              text_normalize_punctuation_stem(a))\n",
    "                                           for a in considered_aliases]),\n",
    "                    'Predicted Alias': predicted['name'],\n",
    "                    'Strategy': strategy,\n",
    "                    'Question': row['question'],\n",
    "                })\n",
    "            break\n",
    "            \n",
    "    if len(candidate_aliases) == 0:\n",
    "        candidates_mids.append([])\n",
    "\n",
    "evaluate_candidates(candidates_mids)\n",
    "print('Negative Sample:')\n",
    "# To not overfit on the first 50 samples\n",
    "print(format_pipe_table(negative_samples[50:100], columns=['Strategy', 'Max Similarity',\n",
    "                                                        'Preprocessed Subject Name',\n",
    "                                                        'Predicted Alias',\n",
    "                                                        'Considered Aliases', 'Question']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "##### Numbers:\n",
    "\n",
    "Version 0\n",
    "- Precision: 0.964420 [10246 of 10624]\n",
    "- Recall: 0.997746 [10624 of 10648]\n",
    "- Expected Guessing Accuracy: 0.659801 [7025 of 10648]\n",
    "\n",
    "Version 1\n",
    "- Precision: 0.968524 [10308 of 10643]\n",
    "- Recall: 0.999530 [10643 of 10648]\n",
    "- Expected Guessing Accuracy: 0.664496 [7075 of 10648]\n",
    "    \n",
    "Verison 2\n",
    "- Precision: 0.973420 [10364 of 10647]\n",
    "- Recall: 0.999906 [10647 of 10648]\n",
    "- Expected Guessing Accuracy: 0.669337 [7127 of 10648]\n",
    "\n",
    "Recall increased by 0.000376.\n",
    "\n",
    "Precision increased by 0.004896.\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "We managed to increase our expected accuracy by a 1% with these simple normalization measures. We believe this will lead to a 1% + gain downstream.\n",
    "\n",
    "##### Error Bucket:\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "For the most part, the wrong span is selected; therefore, the alias is difficult to link with the current implementation of the algorithm.\n",
    "\n",
    "We can handle the similar case some what. The similarity between the correct alias and the subject name tends to be fairly high; therefore, we can try adding a step to look for any aliases that are similar with a score of 0.85+. We expect this to handle  3 / 50 errors.\n",
    "\n",
    "| Max Similarity | Bucket |\n",
    "| --- | --- |\n",
    "| 0.888889 | Similar |\n",
    "| 0.714286 | Similar |\n",
    "| 0.875 | Similar |\n",
    "| 1.0 | Similar |\n",
    "| 0.62069 | Similar |\n",
    "| 1.0 | Similar |\n",
    "| 0.88 | Similar |\n",
    "\n",
    "**Buckets:**\n",
    "- Wrong Span (42 / 50): The wrong span in the question was selected.\n",
    "- Similar (8 / 50): The correct subject name was similar but not exact to the predicted subject name.\n",
    "- Extra Article (2 / 50): The correct subject name was not linked due to an article.\n",
    "\n",
    "| Index | Max Similarity  | Bucket | Strategy | Preprocessed Subject Name | Predicted Alias | Considered Aliases | Question |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 0 | 0.705882 | Wrong Span | PREPROCESSED | pim fortuyn list | pim fortuyn | ['pim fortuyn'] | what ideology does the pim fortuyn list follow |\n",
    "| 1 | 0.0 | Wrong Span | NORMALIZED_PUNCTUATION_STEM | drug | cleaning | ['cleaning hands', 'for cleaning hands', 'cleaning'] | what drug is used for cleaning hands  |\n",
    "| 2 | 0.444444 | Wrong Span | PREPROCESSED | leather subculture | leather | ['leather'] | what's one event that celebrates the leather subculture |\n",
    "| 3 | 0.333333 | Wrong Span | PREPROCESSED | illinois river | rogue river | ['lower rogue river', 'rogue river'] | in which community does the illinois river confluence with the Lower Rogue river |\n",
    "| 4 | 0.764706 | Wrong Span, Extra Article | PREPROCESSED | need for speed | the need for speed | ['the need for speed'] | what type of film is the need for speed |\n",
    "| 5 | 0.0 | Wrong Span | PREPROCESSED | asteroid | geologist | ['geologist'] | which asteroid was names after an italian geologist? |\n",
    "| 6 | 0.0 | Wrong Span | PREPROCESSED | album | chico debarge | ['chico debarge'] | what is an album by Chico DeBarge? |\n",
    "| 7 | 1.0 | Punctuation | PREPROCESSED | unter null . | unter null | ['unter null'] | what type of book binding is unter null. |\n",
    "| 8 | 0.5 | Wrong Span | NORMALIZED_PUNCTUATION | k - pop | - pop music | ['k - pop music', '- pop music'] | Who is an artist of k-pop music? |\n",
    "| 9 | 0.0454545 | Wrong Span | PREPROCESSED | novel | vladimir nabokov | ['vladimir nabokov'] | Name a novel by Vladimir Nabokov |\n",
    "| 10 | 0.888889 | Similar | PREPROCESSED | millard s drexler | millard | ['millard drexler', 'drexler', 'millard'] | What organization did millard drexler found |\n",
    "| 11 | 0.0 | Wrong Span | PREPROCESSED | musical | israel | ['israel'] | which musical films were broadcasted in israel? |\n",
    "| 12 | 0.0 | Wrong Span | PREPROCESSED | live album | 3oh!3 | ['3oh!3'] | what is the name of the live album by 3oh!3 |\n",
    "| 13 | 0.5 | Wrong Span | PREPROCESSED | epic film | epic | ['epic'] | Name a 1936 epic film  |\n",
    "| 14 | 0.533333 | Wrong Span | PREPROCESSED | roy rogers restaurants | roy rogers | ['roy rogers'] | roy rogers restaurants in which industry? |\n",
    "| 15 | 0.0 | Wrong Span | PREPROCESSED | caucasian | babylon 5 | ['babylon 5'] | who is of caucasian race in babylon 5 |\n",
    "| 16 | 0.6 | Wrong Span | PREPROCESSED | painting | visual art | ['visual art painting', 'art painting', 'visual art'] | what is a visual art painting |\n",
    "| 17 | 0.0 | Wrong Span | PREPROCESSED | album | funk | ['koul funk', 'funk'] | what album was release by Koul Funk |\n",
    "| 18 | 0.0555556 | Wrong Span | PREPROCESSED | science | brian swimme | ['brian swimme'] | brian swimme wrote what book that dealt with  science |\n",
    "| 19 | 0.0 | Wrong Span | PREPROCESSED | animation | raoul servais | ['raoul servais'] | Which animation did Raoul Servais direct |\n",
    "| 20 | 0.333333 | Wrong Span | PREPROCESSED | working title films | films | ['films'] | what is the film from the production company working title films |\n",
    "| 21 | 0.0344828 | Wrong Span | PREPROCESSED | jerry bruckheimer | biographical film | ['biographical film'] | jerry bruckheimer was the producer of this biographical film.  |\n",
    "| 22 | 0.714286 | Similar | PREPROCESSED | godbout v longueuil ( city of ) | longueuil | ['godbout v. longueuil', 'godbout v.', 'v. longueuil', 'the godbout v. longueuil', 'godbout v. longueuil case', 'godbout', 'longueuil'] | what court handled the godbout v. longueuil case? |\n",
    "| 23 | 0.875 | Similar | PREPROCESSED | raymond a. meier | raymond | ['raymond meier', 'meier', 'was raymond meier', 'raymond'] | Which city was raymond meier born in |\n",
    "| 24 | 0.714286 | Wrong Span | PREPROCESSED | afterglow | the afterglow | ['the afterglow fil', 'the afterglow fil ,', 'afterglow fil', 'afterglow fil ,', 'the afterglow'] | who did the music for the afterglow fil, |\n",
    "| 25 | 0.0 | Wrong Span | PREPROCESSED | album | leo sayer | ['leo sayer'] | what is an album by leo sayer? |\n",
    "| 26 | 1.0 | Similar | PREPROCESSED | drums | drum | ['drum'] | who played the drum  in the Los Angeles rock quintet Rooney |\n",
    "| 27 | 0.625 | Wrong Span | PREPROCESSED | latin pop | pop music | ['latin pop music', 'pop music'] | who is an artist that creates latin pop music |\n",
    "| 28 | 0.0909091 | Wrong Span | PREPROCESSED | death eaters | harry potter | ['harry potter'] | which is the name of a death eater in harry potter? |\n",
    "| 29 | 0.62069 | Similar | NORMALIZED_PUNCTUATION | single - player video game | single - | ['single - player mode', 'single - player mode game', 'single - player', 'single -'] | what is a single-player mode game? |\n",
    "| 30 | 0.461538 | Wrong Span | PREPROCESSED | album | compilation album | ['compilation album'] | What is a compilation album from 2006  |\n",
    "| 31 | 0.047619 | Wrong Span | PREPROCESSED | studio album | arcangel | ['arcangel'] | What was a studio album recording for Arcángel |\n",
    "| 32 | 0.6 | Wrong Span | PREPROCESSED | saint | the saint | ['the saint novel', 'saint novel', 'the saint'] | What type of book is the saint novel? |\n",
    "| 33 | 0.5 | Wrong Span | PREPROCESSED | avila place | avila | ['avila'] | what western state does contain avila place |\n",
    "| 34 | 0.0357143 | Wrong Span | PREPROCESSED | north carolina | surrey | ['surrey county', 'surrey'] | What is a city in Surrey County, north carolina? |\n",
    "| 35 | 0.0 | Wrong Span | PREPROCESSED | album | johannes brahms | ['johannes brahms'] | What is an album written by Johannes Brahms |\n",
    "| 36 | 0.777778 | Wrong Span | PREPROCESSED | first battle of james island | james island | ['battle of james island', 'battle of james', 'of james island', 'james island'] | Name a soldier involved in the battle of james island. |\n",
    "| 37 | 0.642857 | Wrong Span | PREPROCESSED | plymouth | plymouth rock | ['plymouth rock'] | is there another attraction in plymouth other than plymouth rock |\n",
    "| 38 | 0.705882 | Wrong Span | PREPROCESSED | altered beast | beast | ['beast game', 'altered beast game', 'beast'] | who is the creator of the altered beast game |\n",
    "| 39 | 0.45 | Wrong Span | PREPROCESSED | the barefoot artist | barefoot | ['barefoot'] | which film created the barefoot artist  |\n",
    "| 40 | 0.352941 | Wrong Span | PREPROCESSED | pornographic actor | actor | ['actor'] | who is a pornographic actor |\n",
    "| 41 | 0.0 | Wrong Span | PREPROCESSED | album | century media | ['century media'] | which albums were released by the century media label? |\n",
    "| 42 | 0.5 | Wrong Span | PREPROCESSED | 8833 acer | acer | ['acer'] | what is a 8833 acer |\n",
    "| 43 | 1.0 | Similar | PREPROCESSED | cruisin ' | cruisin | ['cruisin'] | What release is cruisin on? |\n",
    "| 44 | 0.88 | Similar | NORMALIZED_PUNCTUATION_STEM | the wonderful wizard of ha 's | the wonderful | ['the wonderful wizard of', 'the wonderful wizard of has', 'the wonderful wizard', 'wonderful wizard of', 'wonderful wizard of has', 'wonderful wizard', 'wizard of', 'the wonderful'] | What film series is the wonderful wizard of has from? |\n",
    "| 45 | 0.842105 | Wrong Span, Article | PREPROCESSED | tower of london | the tower of london | ['the tower of london'] | who recorded the tower of london |\n",
    "| 46 | 0.7 | Wrong Span | PREPROCESSED | outside in | outside | ['outside'] | Which genre is outside in associated with |\n",
    "| 47 | 0.4 | Wrong Span | PREPROCESSED | rca | rca records | ['rca records'] | Who is an artist  signed by rca records? |\n",
    "| 48 | 0.0 | Wrong Span | PREPROCESSED | action game | sega | ['sega'] | What's an action game made by sega |\n",
    "| 49 | 0.647059 | Wrong Span | PREPROCESSED | film adaptation | novel | ['novel film adaptation', 'novel'] | What's an example of a novel film adaptation |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3\n",
    "\n",
    "In version 2, the error bucketing revealed a failure to handling similar aliases's; therefore, we proceed to handle them in version 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=65536)\n",
    "def cached_similar_alias_normalized_punctuation_stem_to_alias(text):\n",
    "    cursor.execute(\"\"\"SELECT set_limit(0.85);\n",
    "                    SELECT DISTINCT alias FROM fb_two_subject_name \n",
    "                  WHERE alias_normalized_punctuation_stem %% %s\"\"\", (text,))\n",
    "    return list([r[0] for r in cursor.fetchall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b503013e32412d8e3caa045dc19ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10648), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 0.974171 [10372 of 10647]\n",
      "Recall: 0.999906 [10647 of 10648]\n",
      "Expected Guessing Accuracy: 0.669558 [7129 of 10648]\n",
      "Average Number of Aliases: 1.0379413974455296\n"
     ]
    }
   ],
   "source": [
    "from scripts.utils.table import format_pipe_table\n",
    "\n",
    "candidates_mids = []\n",
    "n_aliases = 0\n",
    "\n",
    "for index, row in tqdm_notebook(df_answerable.iterrows(), total=df_answerable.shape[0]):\n",
    "    for i, predicted in enumerate(row['predicted_subject_names']):\n",
    "        candidate_aliases = cached_alias_preprocessed_to_alias(predicted['name'])\n",
    "        \n",
    "        # Punctuation Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            candidate_aliases = cached_alias_normalized_punctuation_to_alias(\n",
    "                text_normalize_punctuation(predicted['name']))\n",
    "            \n",
    "        # Suffix Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            candidate_aliases = cached_alias_normalized_punctuation_stem_to_alias(\n",
    "                text_normalize_punctuation_stem(predicted['name']))\n",
    "        \n",
    "        # Other Similar Aliases\n",
    "        if len(candidate_aliases) == 0:\n",
    "            candidate_aliases = cached_similar_alias_normalized_punctuation_stem_to_alias(\n",
    "                text_normalize_punctuation_stem(predicted['name']))\n",
    "    \n",
    "        if len(candidate_aliases) > 0:\n",
    "            candidates_mids.append(cached_aliases_to_mids(candidate_aliases))\n",
    "            n_aliases += len(candidate_aliases)\n",
    "            break\n",
    "            \n",
    "    if len(candidate_aliases) == 0:\n",
    "        candidates_mids.append([])\n",
    "\n",
    "evaluate_candidates(candidates_mids)\n",
    "print('Average Number of Aliases:', n_aliases / len(candidates_mids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "##### Numbers:\n",
    "\n",
    "Version 0\n",
    "- Precision: 0.964420 [10246 of 10624]\n",
    "- Recall: 0.997746 [10624 of 10648]\n",
    "- Expected Guessing Accuracy: 0.659801 [7025 of 10648]\n",
    "\n",
    "Version 1\n",
    "- Precision: 0.968524 [10308 of 10643]\n",
    "- Recall: 0.999530 [10643 of 10648]\n",
    "- Expected Guessing Accuracy: 0.664496 [7075 of 10648]\n",
    "    \n",
    "Verison 2\n",
    "- Precision: 0.973420 [10364 of 10647]\n",
    "- Recall: 0.999906 [10647 of 10648]\n",
    "- Expected Guessing Accuracy: 0.669337 [7127 of 10648]\n",
    "    \n",
    "Version 3\n",
    "- Precision: 0.974171 [10372 of 10647]\n",
    "- Recall: 0.999906 [10647 of 10648]\n",
    "- Expected Guessing Accuracy: 0.669558 [7129 of 10648]\n",
    "- Average Number Of Aliases: 1.0379413974455296\n",
    "\n",
    "Recall stayed the same.\n",
    "Precision increased by 0.000751.\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "The increase here is small; therefore, it may not be worth it to include this last step in the pipeline. Without this last step, there is little room to grow otherwise with SQL queries. \"Average Number of Aliases\" does indicate that there is some room to grow in filtering out aliases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 4\n",
    "\n",
    "In Version 4, we investigate alias filtering. We also remove the \"suffix differences\" from our algorithm finding that it has no affect after alias filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6385e2810e5c4a32a1f3d658c1aa2657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10648), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 0.972387 [10353 of 10647]\n",
      "Recall: 0.999906 [10647 of 10648]\n",
      "Expected Guessing Accuracy: 0.676108 [7199 of 10648]\n",
      "Average number of alaises: 1.000939143501127\n"
     ]
    }
   ],
   "source": [
    "from Levenshtein import distance\n",
    "from scripts.utils.table import format_pipe_table\n",
    "\n",
    "candidates_mids = []\n",
    "n_aliases = 0\n",
    "\n",
    "for index, row in tqdm_notebook(df_answerable.iterrows(), total=df_answerable.shape[0]):\n",
    "    for i, predicted in enumerate(row['predicted_subject_names']):\n",
    "        candidate_aliases = cached_alias_preprocessed_to_alias(predicted['name'])\n",
    "        \n",
    "        # Punctuation Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            candidate_aliases = cached_alias_normalized_punctuation_to_alias(\n",
    "                text_normalize_punctuation(predicted['name']))\n",
    "        \n",
    "        # Other Similar Aliases\n",
    "        if len(candidate_aliases) == 0:\n",
    "            candidate_aliases = cached_similar_alias_normalized_punctuation_stem_to_alias(\n",
    "                text_normalize_punctuation_stem(predicted['name']))\n",
    "\n",
    "        if len(candidate_aliases) > 0:\n",
    "            # Filter by smallest edit distance to originally predicted name\n",
    "            score = lambda a: (distance(a, predicted['name']), len(a))\n",
    "            best_score = min([score(a) for a in candidate_aliases])\n",
    "            candidate_aliases = [a for a in candidate_aliases if score(a) == best_score]\n",
    "            \n",
    "            # Copute the number of aliases\n",
    "            n_aliases += len(candidate_aliases)\n",
    "            \n",
    "            candidates_mids.append(cached_aliases_to_mids(candidate_aliases))\n",
    "            break\n",
    "            \n",
    "    if len(candidate_aliases) == 0:\n",
    "        candidates_mids.append([])\n",
    "\n",
    "evaluate_candidates(candidates_mids)\n",
    "print('Average number of alaises:', n_aliases / len(candidates_mids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "##### Numbers:\n",
    "\n",
    "Version 0\n",
    "- Precision: 0.964420 [10246 of 10624]\n",
    "- Recall: 0.997746 [10624 of 10648]\n",
    "- Expected Guessing Accuracy: 0.659801 [7025 of 10648]\n",
    "\n",
    "Version 1\n",
    "- Precision: 0.968524 [10308 of 10643]\n",
    "- Recall: 0.999530 [10643 of 10648]\n",
    "- Expected Guessing Accuracy: 0.664496 [7075 of 10648]\n",
    "    \n",
    "Verison 2\n",
    "- Precision: 0.973420 [10364 of 10647]\n",
    "- Recall: 0.999906 [10647 of 10648]\n",
    "- Expected Guessing Accuracy: 0.669337 [7127 of 10648]\n",
    "    \n",
    "Version 3\n",
    "- Precision: 0.974171 [10372 of 10647]\n",
    "- Recall: 0.999906 [10647 of 10648]\n",
    "- Expected Guessing Accuracy: 0.669558 [7129 of 10648]\n",
    "- Average Number Of Aliases: 1.0379413974455296\n",
    "\n",
    "Version 4\n",
    "- Precision: 0.972387 [10353 of 10647]\n",
    "- Recall: 0.999906 [10647 of 10648]\n",
    "- Expected Guessing Accuracy: 0.676108 [7199 of 10648]\n",
    "- Average number of alaises: 1.000939143501127\n",
    "\n",
    "Recall stayed the same.\n",
    "Precision decreased by 0.001784.\n",
    "The expected accuracy went up 0.00655 by close to half a percent.\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "With a small decrease in percision, we were able to reduce the number of aliases to choose from. Resulting in a 0.65% increase in our expected accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin\n",
    "\n",
    "Here we use our algorithm to generate candidates and save the results of Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance\n",
    "from numpy import nan\n",
    "\n",
    "def generate_candidates(cursor, row):\n",
    "    for i, predicted in enumerate(row['predicted_subject_names']):\n",
    "        cursor.execute(\"SELECT DISTINCT alias FROM \" + FB2M_NAME_TABLE + \n",
    "              \" WHERE alias_preprocessed = %s\", (predicted['name'],))\n",
    "        candidate_aliases = list([r[0] for r in cursor.fetchall()])\n",
    "        \n",
    "        # Punctuation Differences\n",
    "        if len(candidate_aliases) == 0:\n",
    "            text = text_normalize_punctuation(predicted['name'])\n",
    "            cursor.execute(\"SELECT DISTINCT alias FROM \" + FB2M_NAME_TABLE + \n",
    "                  \" WHERE alias_normalized_punctuation = %s\", (text,))\n",
    "            candidate_aliases = list([r[0] for r in cursor.fetchall()])\n",
    "        \n",
    "        # Other Similar Aliases\n",
    "        if len(candidate_aliases) == 0:\n",
    "            text = text_normalize_punctuation_stem(predicted['name'])\n",
    "            cursor.execute(\"SELECT DISTINCT alias FROM \" + FB2M_NAME_TABLE + \n",
    "                          \" WHERE alias_normalized_punctuation_stem = %s\", (text,))\n",
    "            candidate_aliases = list([r[0] for r in cursor.fetchall()])\n",
    "\n",
    "        if len(candidate_aliases) > 0:\n",
    "            # Filter by smallest edit distance to originally predicted name\n",
    "            # TODO: Look into filtering after the relation filter\n",
    "            score = lambda a: (distance(a, predicted['name']), len(a))\n",
    "            best_score = min([score(a) for a in candidate_aliases])\n",
    "            candidate_aliases = [a for a in candidate_aliases if score(a) == best_score]\n",
    "            mids = []\n",
    "            for alias in candidate_aliases:\n",
    "                cursor.execute(\"SELECT mid FROM \" + FB2M_NAME_TABLE +  \n",
    "                               \" WHERE alias = %s\", (alias,))\n",
    "                mids.extend(list([r[0] for r in cursor.fetchall()]))\n",
    "            row['candidate_mids'] = mids\n",
    "            row['predicted_start_index'] = predicted['start_index']\n",
    "            row['predicted_end_index'] = predicted['end_index']\n",
    "            row['predicted_subject_name'] = predicted['name']\n",
    "            return row\n",
    "        \n",
    "    row['candidate_mids'] = []\n",
    "    row['predicted_start_index'] = nan\n",
    "    row['predicted_end_index'] = nan\n",
    "    row['predicted_subject_name'] = nan\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a656d0ab84c4262a2f1f40a8999ce94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10845), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ../../scripts/Simple QA Numbers/HYPOTHESIS - Subject Name not in Question.ipynb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "df = df.progress_apply(partial(generate_candidates, cursor), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity\n",
    "\n",
    "Check if `generate_candidates` works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9497edc53f04600b77197960533a48f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10845), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidate Accuracy: 0.954449 [10351 of 10845]\n",
      "Expected Accuracy: 0.663481 [7195 of 10845]\n",
      "Subject Name Accuracy: 0.931581 [10103 of 10845]\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy just to check the implementation of `generate_candidates`.\n",
    "correct = 0\n",
    "expected_correct = 0\n",
    "subject_name_correct = 0\n",
    "for index, row in tqdm_notebook(df.iterrows(), total=df.shape[0]):\n",
    "    if row['subject'] in row['candidate_mids']:\n",
    "        correct += 1\n",
    "        expected_correct += 1 / len(row['candidate_mids'])\n",
    "    \n",
    "    if (isinstance(row['subject_name'], str) and\n",
    "        text_preprocess(row['subject_name']) == row['predicted_subject_name']):\n",
    "        subject_name_correct += 1\n",
    "        \n",
    "print('Candidate Accuracy: %f [%d of %d]' % (correct / df.shape[0], correct, df.shape[0]))\n",
    "print('Expected Accuracy: %f [%d of %d]' % (expected_correct / df.shape[0], expected_correct, df.shape[0]))\n",
    "# TODO: Look at subject names that are incorrect but the subject is correct\n",
    "# Because that's weird.\n",
    "print('Subject Name Accuracy: %f [%d of %d]' %\n",
    "      (subject_name_correct / df.shape[0],subject_name_correct, df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to Pipeline\n",
    "\n",
    "Write step 2 results to use them in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('step_2_generate_candidates.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
